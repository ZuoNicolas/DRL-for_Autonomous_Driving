{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbcbf6ad",
   "metadata": {},
   "source": [
    "## Presentation for the application of deep reinforcement learning for autonomous driving\n",
    "\n",
    "##### Important this project was realized with Python 3.7.9 and with the last version of the libraries used\n",
    "##### With the Windows 0.9.12 version of Carla, although there should be no problem with other versions and linux versions under Ubuntu, but not yet tested!\n",
    "https://github.com/carla-simulator/carla/releases/tag/0.9.12/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75fdad0",
   "metadata": {},
   "source": [
    "Please fill in the PATH_CARLA_EGG, your access path to this file which is normally located in Carla/PythonAPI/carla/dist/carla-*%d.%d-%s.egg when installing carla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c04a782",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialization of my path\n",
    "PATH_CARLA_EGG = '../DRL/Carla/PythonAPI/carla/dist/carla-*%d.%d-%s.egg'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1be216",
   "metadata": {},
   "source": [
    "Import of libraries and initialization of paths for the connection with Carla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "634eb138",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import math\n",
    "import gym\n",
    "\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box \n",
    "\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "#connection with the egg file\n",
    "try:\n",
    "    sys.path.append(glob.glob(PATH_CARLA_EGG % (\n",
    "        sys.version_info.major,\n",
    "        sys.version_info.minor,\n",
    "        'win-amd64' if os.name == 'nt' else 'linux-x86_64'))[0])\n",
    "except IndexError:\n",
    "    pass\n",
    "import carla"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd87ce7",
   "metadata": {},
   "source": [
    "### First version of my agent's environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95f3ead1",
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOW_PREVIEW = False\n",
    "IM_WIDTH = 640\n",
    "IM_HEIGHT = 480\n",
    "MAX_EPISODE = 10\n",
    "\n",
    "\n",
    "class CarEnv:\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    SHOW_CAM = SHOW_PREVIEW\n",
    "    STEER_AMT = 1.0\n",
    "    im_width = IM_WIDTH\n",
    "    im_height = IM_HEIGHT\n",
    "    front_camera = None\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialization of Carla client and environment components\"\"\"\n",
    "        \n",
    "        self.client = carla.Client(\"localhost\", 2000)\n",
    "        self.client.set_timeout(20.0)\n",
    "        self.world = self.client.get_world()\n",
    "        self.blueprint_library = self.world.get_blueprint_library()\n",
    "        self.model_3 = self.blueprint_library.filter(\"model3\")[0]\n",
    "        self.actor_list = []\n",
    "        self.action_space = Discrete(3)\n",
    "        self.observation_space = Box(0, 255, [IM_HEIGHT,IM_WIDTH,3])\n",
    "        self.info = {'episode':0}\n",
    "        self.iter = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.collision_hist = []\n",
    "        self.actor_list = []\n",
    "        self.info = {'episode':0}\n",
    "        self.iter = 0\n",
    "        \n",
    "        #reset the position of our vehicle\n",
    "        self.transform = random.choice(self.world.get_map().get_spawn_points())\n",
    "        self.vehicle = self.world.spawn_actor(self.model_3, self.transform)\n",
    "        self.actor_list.append(self.vehicle)\n",
    "        \n",
    "        #reset the front rgb camera of the vehicle\n",
    "        self.rgb_cam = self.blueprint_library.find('sensor.camera.rgb')\n",
    "        self.rgb_cam.set_attribute(\"image_size_x\", f\"{self.im_width}\")\n",
    "        self.rgb_cam.set_attribute(\"image_size_y\", f\"{self.im_height}\")\n",
    "        self.rgb_cam.set_attribute(\"fov\", f\"110\")\n",
    "\n",
    "        transform = carla.Transform(carla.Location(x=2.5, z=0.7))\n",
    "        not_spawn = True\n",
    "        \n",
    "        #The cases where our vehicle does not arrive to spawn, \n",
    "        #because there is already another agent on the place of the spawn\n",
    "        while not_spawn:\n",
    "            try :\n",
    "                self.sensor = self.world.spawn_actor(self.rgb_cam, transform, attach_to=self.vehicle)\n",
    "                not_spawn = False\n",
    "            except Exception:\n",
    "                not_spawn = True\n",
    "            \n",
    "        self.actor_list.append(self.sensor)\n",
    "        self.sensor.listen(lambda data: self.process_img(data))\n",
    "\n",
    "        self.vehicle.apply_control(carla.VehicleControl(throttle=0.0, brake=0.0))\n",
    "        #time.sleep(5)\n",
    "    \n",
    "        #reset of the collision sensor\n",
    "        colsensor = self.blueprint_library.find(\"sensor.other.collision\")\n",
    "        self.colsensor = self.world.spawn_actor(colsensor, transform, attach_to=self.vehicle)\n",
    "        self.actor_list.append(self.colsensor)\n",
    "        self.colsensor.listen(lambda event: self.collision_data(event))\n",
    "    \n",
    "        #as long as we have not finished converting our camera data into RGB data\n",
    "        while self.front_camera is None:\n",
    "            time.sleep(0.01)\n",
    "\n",
    "        self.episode_start = time.time()\n",
    "        self.vehicle.apply_control(carla.VehicleControl(throttle=0.0, brake=0.0))\n",
    "\n",
    "        return self.front_camera\n",
    "\n",
    "    def collision_data(self, event):\n",
    "        self.collision_hist.append(event)\n",
    "\n",
    "    def process_img(self, image):\n",
    "        \"\"\"conversion of the raw image that Carla sends us from the RGB sensor, \n",
    "        into data of type (HEIGHT, WIDTH, 3)\"\"\"\n",
    "        \n",
    "        i = np.array(image.raw_data)\n",
    "        #print(i.shape)\n",
    "        i2 = i.reshape((self.im_height, self.im_width, 4))\n",
    "        i3 = i2[:, :, :3]\n",
    "        if self.SHOW_CAM:\n",
    "            cv2.imshow(\"\", i3)\n",
    "            cv2.waitKey(1)\n",
    "        self.front_camera = i3\n",
    "\n",
    "    def step(self, action):\n",
    "        if action == 0:\n",
    "            #turn left\n",
    "            self.vehicle.apply_control(carla.VehicleControl(throttle=1.0, steer=-1*self.STEER_AMT))\n",
    "        elif action == 1:\n",
    "            #continue straight on\n",
    "            self.vehicle.apply_control(carla.VehicleControl(throttle=1.0, steer= 0))\n",
    "        elif action == 2:\n",
    "            #turn left\n",
    "            self.vehicle.apply_control(carla.VehicleControl(throttle=1.0, steer=1*self.STEER_AMT))\n",
    "\n",
    "        #conversion to km/h\n",
    "        v = self.vehicle.get_velocity()\n",
    "        kmh = int(3.6 * math.sqrt(v.x**2 + v.y**2 + v.z**2))\n",
    "\n",
    "        if len(self.collision_hist) != 0:\n",
    "            done = True\n",
    "            reward = -100\n",
    "        elif kmh < 40:\n",
    "            done = False\n",
    "            reward = -1\n",
    "        else:\n",
    "            done = False\n",
    "            reward = 1\n",
    "\n",
    "        if self.iter > MAX_EPISODE-1:\n",
    "            done = True\n",
    "        self.info['episode'] += 1\n",
    "        self.iter += 1\n",
    "        \n",
    "        return self.front_camera , reward, done, self.info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ad0240",
   "metadata": {},
   "source": [
    "### Implementation of my DQN module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0784eacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQNetwork(nn.Module):\n",
    "    def __init__(self, lr, input_dims, fc1_dims, fc2_dims, n_actions):\n",
    "        \"\"\"Initialization of a simple neural network with 3 layers, of MLP type\n",
    "        and MSE loss\"\"\"\n",
    "        \n",
    "        super(DeepQNetwork, self).__init__()\n",
    "        self.input_dims = input_dims\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims\n",
    "        self.n_actions = n_actions\n",
    "        self.fc1 = nn.Linear(*self.input_dims, self.fc1_dims)\n",
    "        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
    "        self.fc3 = nn.Linear(self.fc2_dims, self.n_actions)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        self.loss = nn.MSELoss()\n",
    "        \n",
    "        #using the gpu\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"activation relu with the following possible actions\"\"\"\n",
    "        \n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        actions = self.fc3(x)\n",
    "\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d47fb7",
   "metadata": {},
   "source": [
    "### Implementation of my agent module\n",
    "\n",
    "Initialization of the DQN module, to learn the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4637bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, gamma, epsilon, lr, input_dims, batch_size, n_actions,\n",
    "                 max_mem_size=100, eps_end=0.05, eps_dec=5e-4):\n",
    "        \"\"\"Initialization of the hyperparameters \n",
    "            for learning and the neural network, \n",
    "            with a memory system\"\"\"\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.eps_min = eps_end\n",
    "        self.eps_dec = eps_dec\n",
    "        self.lr = lr\n",
    "        self.action_space = [i for i in range(n_actions)]\n",
    "        self.mem_size = max_mem_size\n",
    "        self.batch_size = batch_size\n",
    "        self.mem_cntr = 0\n",
    "        self.iter_cntr = 0\n",
    "        self.replace_target = 100\n",
    "\n",
    "        #initialization of the NN\n",
    "        self.Q_eval = DeepQNetwork(lr, n_actions=n_actions,\n",
    "                                   input_dims=input_dims,\n",
    "                                   fc1_dims=256, fc2_dims=256)\n",
    "        \n",
    "        self.state_memory = np.zeros((self.mem_size, *input_dims),\n",
    "                                     dtype=np.float32)\n",
    "        self.new_state_memory = np.zeros((self.mem_size, *input_dims),\n",
    "                                         dtype=np.float32)\n",
    "        self.action_memory = np.zeros(self.mem_size, dtype=np.int32)\n",
    "        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.bool)\n",
    "        \n",
    "        #For tensorboard\n",
    "        self.writer = SummaryWriter()\n",
    "        \n",
    "        self.n_iter = 0\n",
    "\n",
    "    def store_transition(self, state, action, reward, state_, terminal):\n",
    "        index = self.mem_cntr % self.mem_size\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = state_\n",
    "        self.reward_memory[index] = reward\n",
    "        self.action_memory[index] = action\n",
    "        self.terminal_memory[index] = terminal\n",
    "\n",
    "        self.mem_cntr += 1\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        #espsilon for the exploration rate\n",
    "        if np.random.random() > self.epsilon:\n",
    "            state = T.tensor([observation]).to(self.Q_eval.device)\n",
    "            actions = self.Q_eval.forward(state.float())\n",
    "            action = T.argmax(actions).item()\n",
    "        else:\n",
    "            action = np.random.choice(self.action_space)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def learn(self):\n",
    "        if self.mem_cntr < self.batch_size:\n",
    "            return\n",
    "\n",
    "        self.Q_eval.optimizer.zero_grad()\n",
    "\n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "\n",
    "        batch = np.random.choice(max_mem, self.batch_size, replace=False)\n",
    "        batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "        \n",
    "        #classic initialization for CUDA\n",
    "        state_batch = T.tensor(self.state_memory[batch]).to(self.Q_eval.device)\n",
    "        new_state_batch = T.tensor(self.new_state_memory[batch]).to(self.Q_eval.device)\n",
    "        action_batch = self.action_memory[batch]\n",
    "        reward_batch = T.tensor(self.reward_memory[batch]).to(self.Q_eval.device)\n",
    "        terminal_batch = T.tensor(self.terminal_memory[batch]).to(self.Q_eval.device)\n",
    "        \n",
    "        #the forward step\n",
    "        q_eval = self.Q_eval.forward(state_batch)[batch_index, action_batch]\n",
    "        q_next = self.Q_eval.forward(new_state_batch)\n",
    "        q_next[terminal_batch] = 0.0\n",
    "\n",
    "        q_target = reward_batch + self.gamma*T.max(q_next, dim=1)[0]\n",
    "\n",
    "        #loss calculation\n",
    "        loss = self.Q_eval.loss(q_target, q_eval).to(self.Q_eval.device)\n",
    "        \n",
    "        #the backward step and weight update\n",
    "        loss.backward()\n",
    "        self.Q_eval.optimizer.step()\n",
    "        \n",
    "        self.iter_cntr += 1\n",
    "        #manual update of the exploration rate\n",
    "        self.epsilon = self.epsilon - self.eps_dec \\\n",
    "            if self.epsilon > self.eps_min else self.eps_min\n",
    "        \n",
    "        #loss display in tensorboard\n",
    "        self.writer.add_scalar('Loss/', loss, self.n_iter)\n",
    "        self.n_iter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b69731",
   "metadata": {},
   "source": [
    "### Start learning\n",
    "\n",
    "Runtime can take up to 1h, if you have a GPU similar to mine, i.e. Nvidia gtx 1060 ti\n",
    "\n",
    "If you don't want to start the training, you have the possibility to load a model already trained further down in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bfa02971",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9132/130534009.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCarEnv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m agent = Agent(gamma=0.99, epsilon=1.0, batch_size=5, n_actions=3, eps_end=0.1,\n\u001b[0;32m      3\u001b[0m               input_dims=[480*640*3], lr=0.001, eps_dec=1e-3)\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9132/775906056.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_timeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mworld\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_world\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblueprint_library\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mworld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_blueprint_library\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: time-out of 20000ms while waiting for the simulator, make sure the simulator is ready and connected to localhost:2000",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = CarEnv()\n",
    "agent = Agent(gamma=0.99, epsilon=1.0, batch_size=5, n_actions=3, eps_end=0.1,\n",
    "              input_dims=[480*640*3], lr=0.001, eps_dec=1e-3)\n",
    "scores, eps_history = [], []\n",
    "\n",
    "#number of iterations\n",
    "n_games = 300\n",
    "render = False\n",
    "\n",
    "start_time = time.time()\n",
    "for i in range(n_games):\n",
    "    #initialization of the episode\n",
    "    score = 0\n",
    "    done = False\n",
    "    #get the first observation\n",
    "    observation = env.reset()\n",
    "    #normalization of observations and transformation into a simple list\n",
    "    flat_observation = observation.reshape(1,-1)[0]/255.0\n",
    "    \n",
    "    while not done:\n",
    "        if render:\n",
    "            cv2.imshow(f'Agent - preview', observation)\n",
    "            cv2.waitKey(1)\n",
    "            \n",
    "        action = agent.choose_action(flat_observation.astype(float))\n",
    "        observation_, reward, done, info = env.step(action)\n",
    "        flat_observation_ = observation_.reshape(1,-1)[0]/255.0\n",
    "        score += reward\n",
    "        agent.store_transition(flat_observation.astype(float), action, reward, \n",
    "                                flat_observation_, done)\n",
    "        agent.learn()\n",
    "        flat_observation = flat_observation_\n",
    "        observation = observation_\n",
    "    scores.append(score)\n",
    "    eps_history.append(agent.epsilon)\n",
    "    \n",
    "    #average of the last 10 scores\n",
    "    avg_score = np.mean(scores[-10:])\n",
    "    if render:\n",
    "        cv2.destroyWindow(f'Agent - preview')\n",
    "    for actor in env.actor_list:\n",
    "        actor.destroy()\n",
    "    time_n = time.time() - start_time\n",
    "    print('episode ', i, 'score %.2f' % score,\n",
    "            'average score %.2f' % avg_score,\n",
    "            'epsilon %.2f' % agent.epsilon,\n",
    "             'time %.2f s' % time_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7879ea24",
   "metadata": {},
   "source": [
    "## New environment module\n",
    "\n",
    "Recalculation of the reward function, which rewards according to the distance traveled between the starting point and the current point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25de4bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOW_PREVIEW = False\n",
    "IM_WIDTH = 640\n",
    "IM_HEIGHT = 480\n",
    "MAX_EPISODE = 10\n",
    "\n",
    "\n",
    "class CarEnvDistanceReward:\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    SHOW_CAM = SHOW_PREVIEW\n",
    "    STEER_AMT = 1.0\n",
    "    im_width = IM_WIDTH\n",
    "    im_height = IM_HEIGHT\n",
    "    front_camera = None\n",
    "\n",
    "    def __init__(self, reward_function=None, verbose=0, EXPERIENCE_SECONDE = None):\n",
    "        \"\"\"Initialization of Carla client and environment components\"\"\"\n",
    "\n",
    "        self.client = carla.Client(\"localhost\", 2000)\n",
    "        self.client.set_timeout(20.0)\n",
    "        self.world = self.client.get_world()\n",
    "        self.blueprint_library = self.world.get_blueprint_library()\n",
    "        self.model_3 = self.blueprint_library.filter(\"model3\")[0]\n",
    "        self.actor_list = []\n",
    "        self.action_space = Discrete(3)\n",
    "        self.observation_space = Box(0, 255, [IM_HEIGHT,IM_WIDTH,3])\n",
    "        self.info = {'episode':0}\n",
    "        self.reward_function = reward_function\n",
    "        self.verbose = verbose\n",
    "        self.iter = 0\n",
    "        self.experience_seconde = EXPERIENCE_SECONDE\n",
    "\n",
    "    def reset(self):\n",
    "        \n",
    "        self.collision_hist = []\n",
    "        self.actor_list = []\n",
    "        self.info = {'episode':0}\n",
    "        self.iter = 0\n",
    "        \n",
    "        #reset the position of our vehicle\n",
    "        self.transform = random.choice(self.world.get_map().get_spawn_points())\n",
    "        self.vehicle = self.world.spawn_actor(self.model_3, self.transform)\n",
    "        self.actor_list.append(self.vehicle)\n",
    "\n",
    "        #reset the front rgb camera of the vehicle\n",
    "        self.rgb_cam = self.blueprint_library.find('sensor.camera.rgb')\n",
    "        self.rgb_cam.set_attribute(\"image_size_x\", f\"{self.im_width}\")\n",
    "        self.rgb_cam.set_attribute(\"image_size_y\", f\"{self.im_height}\")\n",
    "        self.rgb_cam.set_attribute(\"fov\", f\"110\")\n",
    "\n",
    "        transform = carla.Transform(carla.Location(x=2.5, z=0.7))\n",
    "        not_spawn = True\n",
    "\n",
    "        #The cases where our vehicle does not arrive to spawn, \n",
    "        #because there is already another agent on the place of the spawn\n",
    "        while not_spawn:\n",
    "            try :\n",
    "                self.sensor = self.world.spawn_actor(self.rgb_cam, transform, attach_to=self.vehicle)\n",
    "                not_spawn = False\n",
    "            except Exception:\n",
    "                not_spawn = True\n",
    "            \n",
    "        self.actor_list.append(self.sensor)\n",
    "        self.sensor.listen(lambda data: self.process_img(data))\n",
    "\n",
    "        self.vehicle.apply_control(carla.VehicleControl(throttle=0.0, brake=0.0))\n",
    "        \n",
    "        #reset of the collision sensor\n",
    "        colsensor = self.blueprint_library.find(\"sensor.other.collision\")\n",
    "        self.colsensor = self.world.spawn_actor(colsensor, transform, attach_to=self.vehicle)\n",
    "        self.actor_list.append(self.colsensor)\n",
    "        self.colsensor.listen(lambda event: self.collision_data(event))\n",
    "        \n",
    "        #as long as we have not finished converting our camera data into RGB data\n",
    "        while self.front_camera is None:\n",
    "            time.sleep(0.01)\n",
    "\n",
    "        self.episode_start = time.time()\n",
    "        self.vehicle.apply_control(carla.VehicleControl(throttle=0.0, brake=0.0))\n",
    "        \n",
    "        #initial position of the vehicle\n",
    "        self.initial_Location = self.vehicle.get_location()\n",
    "        \n",
    "        return self.front_camera\n",
    "\n",
    "    def collision_data(self, event):\n",
    "        self.collision_hist.append(event)\n",
    "\n",
    "    def process_img(self, image):\n",
    "        \"\"\"conversion of the raw image that Carla sends us from the RGB sensor, \n",
    "        into data of type (HEIGHT, WIDTH, 3)\"\"\"\n",
    "        \n",
    "        i = np.array(image.raw_data)\n",
    "        #print(i.shape)\n",
    "        i2 = i.reshape((self.im_height, self.im_width, 4))\n",
    "        i3 = i2[:, :, :3]\n",
    "        if self.SHOW_CAM:\n",
    "            cv2.imshow(\"\", i3)\n",
    "            cv2.waitKey(1)\n",
    "        self.front_camera = i3\n",
    "\n",
    "    def step(self, action):\n",
    "        if action == 0:\n",
    "            #turn left\n",
    "            self.vehicle.apply_control(carla.VehicleControl(throttle=1.0, steer=-1*self.STEER_AMT))\n",
    "        elif action == 1:\n",
    "            #continue straight on\n",
    "            self.vehicle.apply_control(carla.VehicleControl(throttle=1.0, steer= 0))\n",
    "        elif action == 2:\n",
    "             #turn left\n",
    "            self.vehicle.apply_control(carla.VehicleControl(throttle=1.0, steer=1*self.STEER_AMT))\n",
    "\n",
    "        #v = self.vehicle.get_velocity()\n",
    "        #kmh = int(3.6 * math.sqrt(v.x**2 + v.y**2 + v.z**2))\n",
    "\n",
    "        if len(self.collision_hist) != 0:\n",
    "            #if collision\n",
    "            done = True\n",
    "            reward = -50\n",
    "            if self.verbose == 1:\n",
    "                print(\"Collision detected !\")\n",
    "        else :\n",
    "            done = False\n",
    "            #calculation of the distance from the starting point\n",
    "            distance = self.vehicle.get_location().distance(self.initial_Location)\n",
    "            if self.reward_function == None : \n",
    "                reward = distance\n",
    "            else:\n",
    "                reward = self.reward_function(distance)\n",
    "            if self.verbose == 1:\n",
    "                print(\"Obtain \"+str(reward)+\" reward, distance = \"+str(distance))\n",
    "        \n",
    "        if self.experience_seconde != None :\n",
    "            #iteration in second grade\n",
    "            if time.time()-self.episode_start > self.experience_seconde:\n",
    "                done = True\n",
    "        else :\n",
    "            #iteration by number of steps\n",
    "            if self.iter > MAX_EPISODE-1 :\n",
    "                done = True\n",
    "            self.iter += 1\n",
    "        self.info['episode'] += 1\n",
    "        \n",
    "        return self.front_camera , reward, done, self.info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d293f4",
   "metadata": {},
   "source": [
    "##### Test with a reward function that rewards more the fact of covering a greater distance, function of exponential type, for positive x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2f8eda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_function(x):\n",
    "    return (x**3) / 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f68e1b",
   "metadata": {},
   "source": [
    "### Start learning with the new environment\n",
    "\n",
    "Runtime can take up to 1h, if you have a GPU similar to mine, i.e. Nvidia gtx 1060 ti\n",
    "\n",
    "If you don't want to start the training, you have the possibility to load a model already trained further down in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8be0e63e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode  0 score -48.72 average score -48.72 epsilon 1.00 time 8.05 s\n",
      "episode  1 score 76.54 average score 13.91 epsilon 0.99 time 30.10 s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9132/1289842808.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mobservation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mflat_observation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobservation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m255.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9132/2153863239.py\u001b[0m in \u001b[0;36mreset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mnot_spawn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[1;32mtry\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mworld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspawn_actor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrgb_cam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattach_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvehicle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m                 \u001b[0mnot_spawn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = CarEnvDistanceReward()\n",
    "#env = CarEnvDistanceReward(reward_function=reward_function)\n",
    "agent = Agent(gamma=0.99, epsilon=1.0, batch_size=5, n_actions=3, eps_end=0.1,\n",
    "              input_dims=[480*640*3], lr=0.001, eps_dec=1e-3)\n",
    "scores, eps_history = [], []\n",
    "\n",
    "#number of iterations\n",
    "n_games = 300\n",
    "render = False\n",
    "\n",
    "start_time = time.time()\n",
    "for i in range(n_games):\n",
    "    #initialization of the episode\n",
    "    score = 0\n",
    "    done = False\n",
    "    #get the first observation\n",
    "    observation = env.reset()\n",
    "    #normalization of observations and transformation into a simple list\n",
    "    flat_observation = observation.reshape(1,-1)[0]/255.0\n",
    "    while not done:\n",
    "        action = agent.choose_action(flat_observation.astype(float))\n",
    "        if render:\n",
    "            cv2.imshow(f'Agent - preview', observation)\n",
    "            cv2.waitKey(1)\n",
    "        \n",
    "        observation_, reward, done, info = env.step(action)\n",
    "        flat_observation_ = observation_.reshape(1,-1)[0]/255.0\n",
    "        score += reward\n",
    "        agent.store_transition(flat_observation.astype(float), action, reward, \n",
    "                                flat_observation_, done)\n",
    "        agent.learn()\n",
    "        flat_observation = flat_observation_\n",
    "        observation = observation_\n",
    "    scores.append(score)\n",
    "    eps_history.append(agent.epsilon)\n",
    "\n",
    "    avg_score = np.mean(scores[-10:])\n",
    "    if render:\n",
    "        cv2.destroyWindow(f'Agent - preview')\n",
    "    for actor in env.actor_list:\n",
    "        actor.destroy()\n",
    "    time_n = time.time() - start_time\n",
    "    print('episode ', i, 'score %.2f' % score,\n",
    "            'average score %.2f' % avg_score,\n",
    "            'epsilon %.2f' % agent.epsilon,\n",
    "             'time %.2f s' % time_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50835e3",
   "metadata": {},
   "source": [
    "### Display of model data during training\n",
    "\n",
    "(note: You can also see the evolution of the loss, during the training with TensorBoard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8f9ec35",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (300,) and (2,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9132/4095145658.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m \u001b[0mplotLearning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps_history\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9132/4095145658.py\u001b[0m in \u001b[0;36mplotLearning\u001b[1;34m(x, scores, epsilons, filename, lines)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0max2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m111\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"2\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe_on\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilons\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"C0\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_xlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Game\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"C0\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_ylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Epsilon\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"C0\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nico\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1603\u001b[0m         \"\"\"\n\u001b[0;32m   1604\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1605\u001b[1;33m         \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1606\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1607\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nico\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m    313\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m                 \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 315\u001b[1;33m             \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nico\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[1;34m(self, tup, kwargs, return_kwargs)\u001b[0m\n\u001b[0;32m    499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 501\u001b[1;33m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[0;32m    502\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[0;32m    503\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (300,) and (2,)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "start_time = time.time()\n",
    "x = [i+1 for i in range(n_games)]\n",
    "filename = 'data/DQN-EnvDistanceReward-'+str(n_games)+'-'+str(start_time)+'.jpg'\n",
    "\n",
    "def plotLearning(x, scores, epsilons, filename, lines=None):\n",
    "    fig=plt.figure()\n",
    "    ax=fig.add_subplot(111, label=\"1\")\n",
    "    ax2=fig.add_subplot(111, label=\"2\", frame_on=False)\n",
    "\n",
    "    ax.plot(x, epsilons, color=\"C0\")\n",
    "    ax.set_xlabel(\"Game\", color=\"C0\")\n",
    "    ax.set_ylabel(\"Epsilon\", color=\"C0\")\n",
    "    ax.tick_params(axis='x', colors=\"C0\")\n",
    "    ax.tick_params(axis='y', colors=\"C0\")\n",
    "\n",
    "    N = len(scores)\n",
    "    running_avg = np.empty(N)\n",
    "    for t in range(N):\n",
    "        running_avg[t] = np.mean(scores[max(0, t-20):(t+1)])\n",
    "\n",
    "    ax2.plot(x, running_avg, color=\"C1\")\n",
    "\n",
    "    ax2.axes.get_xaxis().set_visible(False)\n",
    "    ax2.yaxis.tick_right()\n",
    "    ax2.set_ylabel('Score', color=\"C1\")\n",
    "    ax2.yaxis.set_label_position('right')\n",
    "    ax2.tick_params(axis='y', colors=\"C1\")\n",
    "\n",
    "    if lines is not None:\n",
    "        for line in lines:\n",
    "            plt.axvline(x=line)\n",
    "\n",
    "    plt.savefig(filename)\n",
    "    \n",
    "plotLearning(x, scores, eps_history, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7ed47c",
   "metadata": {},
   "source": [
    "### Saving the model\n",
    "\n",
    "If you have trained your model before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f336d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "savepath = Path('model/model_DQN_EnvDistanceReward_WithRewardFunction_{}.pch'.format(str(n_games)+'-'+str(start_time)))\n",
    "\n",
    "with savepath.open('wb') as file:\n",
    "    torch.save(agent.Q_eval, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9f335e",
   "metadata": {},
   "source": [
    "### Potential model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac1f14d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "#model learned about the environment with the rewards depending on the distance,\n",
    "#with 600 iterations, the agent has learned to turn in circles\n",
    "#savepath = Path('model/model_DQN_EnvDistanceReward_600-1635352803.4654603.pch')\n",
    "\n",
    "#model learned with the same environment but with the cube function,\n",
    "#where the distance is very important,\n",
    "#the vehicle learned to go straight\n",
    "#savepath = Path('model/model_DQN_EnvDistanceReward_WithRewardFunction_300-1635362488.656119.pch')\n",
    "\n",
    "#model learned about the environment with the rewards depending on the distance,\n",
    "#with 300 iterations, the agent has learned to turn in circles \n",
    "#and the beginning of a collision avoidance behaviour.\n",
    "#but with the delay due to the latency during learning, the rendering is not perfect at all\n",
    "savepath = Path('model/model_DQN_EnvDistanceReward_300-1635331329.0202696.pch')\n",
    "with savepath.open('rb') as file:\n",
    "    model = T.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320c9518",
   "metadata": {},
   "source": [
    "### If the model is already present\n",
    "\n",
    "If you have launched the learning cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c52f91e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = agent.Q_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5350b3e5",
   "metadata": {},
   "source": [
    "## Application of the model learned on the simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dca07b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode  0 score 1002.69 time 10.05 s\n",
      "episode  1 score 2464.82 time 20.35 s\n",
      "episode  2 score 2737.31 time 30.62 s\n",
      "episode  3 score 281.62 time 37.02 s\n",
      "episode  4 score -14.62 time 40.90 s\n"
     ]
    }
   ],
   "source": [
    "episode = 5\n",
    "render = True\n",
    "#env = CarEnv()\n",
    "env = CarEnvDistanceReward(EXPERIENCE_SECONDE= 10)\n",
    "start_time = time.time()\n",
    "for i in range(episode):\n",
    "    score = 0\n",
    "    done = False\n",
    "    observation = env.reset()\n",
    "    flat_observation = observation.reshape(1,-1)[0]/255.0\n",
    "    try : \n",
    "        while not done:\n",
    "\n",
    "            if render:\n",
    "                cv2.imshow(f'Agent - preview', observation)\n",
    "                cv2.waitKey(1)\n",
    "\n",
    "            data = T.tensor(flat_observation).float()\n",
    "\n",
    "            action = model.forward(data)\n",
    "            action = action.detach().numpy().argmax()\n",
    "\n",
    "            observation_, reward, done, info = env.step(action)\n",
    "            flat_observation_ = observation_.reshape(1,-1)[0]/255.0\n",
    "            score += reward\n",
    "            flat_observation = flat_observation_\n",
    "            observation = observation_\n",
    "\n",
    "    finally : \n",
    "        if render:\n",
    "            cv2.destroyWindow(f'Agent - preview')\n",
    "        for actor in env.actor_list:\n",
    "            actor.destroy()\n",
    "        time_n = time.time() - start_time\n",
    "        print('episode ', i, 'score %.2f' % score,'time %.2f s' % time_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7db3464",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
