{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbcbf6ad",
   "metadata": {},
   "source": [
    "# Presentation for the application of deep reinforcement learning for autonomous driving with the Carla simulator\n",
    "\n",
    "##### Important this project was realized with Python 3.7.9 and with the last version of the libraries used\n",
    "##### With the Windows 0.9.12 version of Carla, although there should be no problem with other versions and linux versions under Ubuntu, but not yet tested!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97767a54",
   "metadata": {},
   "source": [
    "This project was done during 1 weekend, and it still needs a lot of changes and improvements. The improvements and changes for the continuation of this project will be :\n",
    "\n",
    "<ol>\n",
    "    <li>Change the neural network to a convolution type network (CNN). Which is much more suitable when using image data as input.</li>\n",
    "    \n",
    "\n",
    "<li>A SSH connection to the faculty computers, rather than a local connection, to avoid latencies (on my PC), because my GPU is not powerful enough to support the Simulator and the model learning with CUDA, which uses all the GPU without limitation. This makes the simulator slow down and create a latency.</li>\n",
    "    \n",
    "\n",
    "<li>Documentation on a way to have a better method to define the reward functions, because until now, it was defined in an arbitrary way. The EPIC method for Equivalent-Policy Invariant Comparison, seems to be a promising way.</li>\n",
    "\n",
    "    \n",
    "<li>Then an optimization of the hypers-parameters, with Optuna and also with the NAS method, to automate the design of artificial neural networks, so that it is optimal !</li>\n",
    "    \n",
    "\n",
    "<li>To test also other type of sensor that Carla proposes, because there we use only the front RGB camera of the vehicle.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75fdad0",
   "metadata": {},
   "source": [
    "**Please fill in the PATH_CARLA_EGG**, your access path to this file which is normally located in Carla/PythonAPI/carla/dist/carla-*%d.%d-%s.egg when installing Carla\n",
    "\n",
    "https://github.com/carla-simulator/carla/releases/tag/0.9.12/\n",
    "\n",
    "And you have to launch the Carla simulator, so that it has the connection with the simulator, otherwise the code will not work !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c04a782",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example, initialization of my path:\n",
    "PATH_CARLA_EGG = '../DRL/Carla/PythonAPI/carla/dist/carla-*%d.%d-%s.egg'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1be216",
   "metadata": {},
   "source": [
    "Import of libraries and initialization of paths for the connection with Carla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634eb138",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import math\n",
    "import gym\n",
    "\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box \n",
    "\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "#connection with the egg file\n",
    "try:\n",
    "    sys.path.append(glob.glob(PATH_CARLA_EGG % (\n",
    "        sys.version_info.major,\n",
    "        sys.version_info.minor,\n",
    "        'win-amd64' if os.name == 'nt' else 'linux-x86_64'))[0])\n",
    "except IndexError:\n",
    "    pass\n",
    "import carla"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd87ce7",
   "metadata": {},
   "source": [
    "### First version of my agent's environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f3ead1",
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOW_PREVIEW = False\n",
    "IM_WIDTH = 640\n",
    "IM_HEIGHT = 480\n",
    "MAX_EPISODE = 10\n",
    "\n",
    "\n",
    "class CarEnv:\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    SHOW_CAM = SHOW_PREVIEW\n",
    "    STEER_AMT = 1.0\n",
    "    im_width = IM_WIDTH\n",
    "    im_height = IM_HEIGHT\n",
    "    front_camera = None\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialization of Carla client and environment components\"\"\"\n",
    "        \n",
    "        self.client = carla.Client(\"localhost\", 2000)\n",
    "        self.client.set_timeout(20.0)\n",
    "        self.world = self.client.get_world()\n",
    "        self.blueprint_library = self.world.get_blueprint_library()\n",
    "        self.model_3 = self.blueprint_library.filter(\"model3\")[0]\n",
    "        self.actor_list = []\n",
    "        self.action_space = Discrete(3)\n",
    "        self.observation_space = Box(0, 255, [IM_HEIGHT,IM_WIDTH,3])\n",
    "        self.info = {'episode':0}\n",
    "        self.iter = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.collision_hist = []\n",
    "        self.actor_list = []\n",
    "        self.info = {'episode':0}\n",
    "        self.iter = 0\n",
    "        \n",
    "        #reset the position of our vehicle\n",
    "        self.transform = random.choice(self.world.get_map().get_spawn_points())\n",
    "        self.vehicle = self.world.spawn_actor(self.model_3, self.transform)\n",
    "        self.actor_list.append(self.vehicle)\n",
    "        \n",
    "        #reset the front rgb camera of the vehicle\n",
    "        self.rgb_cam = self.blueprint_library.find('sensor.camera.rgb')\n",
    "        self.rgb_cam.set_attribute(\"image_size_x\", f\"{self.im_width}\")\n",
    "        self.rgb_cam.set_attribute(\"image_size_y\", f\"{self.im_height}\")\n",
    "        self.rgb_cam.set_attribute(\"fov\", f\"110\")\n",
    "\n",
    "        transform = carla.Transform(carla.Location(x=2.5, z=0.7))\n",
    "        not_spawn = True\n",
    "        \n",
    "        #The cases where our vehicle does not arrive to spawn, \n",
    "        #because there is already another agent on the place of the spawn\n",
    "        while not_spawn:\n",
    "            try :\n",
    "                self.sensor = self.world.spawn_actor(self.rgb_cam, transform, attach_to=self.vehicle)\n",
    "                not_spawn = False\n",
    "            except Exception:\n",
    "                not_spawn = True\n",
    "            \n",
    "        self.actor_list.append(self.sensor)\n",
    "        self.sensor.listen(lambda data: self.process_img(data))\n",
    "\n",
    "        self.vehicle.apply_control(carla.VehicleControl(throttle=0.0, brake=0.0))\n",
    "        #time.sleep(5)\n",
    "    \n",
    "        #reset of the collision sensor\n",
    "        colsensor = self.blueprint_library.find(\"sensor.other.collision\")\n",
    "        self.colsensor = self.world.spawn_actor(colsensor, transform, attach_to=self.vehicle)\n",
    "        self.actor_list.append(self.colsensor)\n",
    "        self.colsensor.listen(lambda event: self.collision_data(event))\n",
    "    \n",
    "        #as long as we have not finished converting our camera data into RGB data\n",
    "        while self.front_camera is None:\n",
    "            time.sleep(0.01)\n",
    "\n",
    "        self.episode_start = time.time()\n",
    "        self.vehicle.apply_control(carla.VehicleControl(throttle=0.0, brake=0.0))\n",
    "\n",
    "        return self.front_camera\n",
    "\n",
    "    def collision_data(self, event):\n",
    "        self.collision_hist.append(event)\n",
    "\n",
    "    def process_img(self, image):\n",
    "        \"\"\"conversion of the raw image that Carla sends us from the RGB sensor, \n",
    "        into data of type (HEIGHT, WIDTH, 3)\"\"\"\n",
    "        \n",
    "        i = np.array(image.raw_data)\n",
    "        #print(i.shape)\n",
    "        i2 = i.reshape((self.im_height, self.im_width, 4))\n",
    "        i3 = i2[:, :, :3]\n",
    "        if self.SHOW_CAM:\n",
    "            cv2.imshow(\"\", i3)\n",
    "            cv2.waitKey(1)\n",
    "        self.front_camera = i3\n",
    "\n",
    "    def step(self, action):\n",
    "        if action == 0:\n",
    "            #turn left\n",
    "            self.vehicle.apply_control(carla.VehicleControl(throttle=1.0, steer=-1*self.STEER_AMT))\n",
    "        elif action == 1:\n",
    "            #continue straight on\n",
    "            self.vehicle.apply_control(carla.VehicleControl(throttle=1.0, steer= 0))\n",
    "        elif action == 2:\n",
    "            #turn left\n",
    "            self.vehicle.apply_control(carla.VehicleControl(throttle=1.0, steer=1*self.STEER_AMT))\n",
    "\n",
    "        #conversion to km/h\n",
    "        v = self.vehicle.get_velocity()\n",
    "        kmh = int(3.6 * math.sqrt(v.x**2 + v.y**2 + v.z**2))\n",
    "\n",
    "        if len(self.collision_hist) != 0:\n",
    "            done = True\n",
    "            reward = -100\n",
    "        elif kmh < 40:\n",
    "            done = False\n",
    "            reward = -1\n",
    "        else:\n",
    "            done = False\n",
    "            reward = 1\n",
    "\n",
    "        if self.iter > MAX_EPISODE-1:\n",
    "            done = True\n",
    "        self.info['episode'] += 1\n",
    "        self.iter += 1\n",
    "        \n",
    "        return self.front_camera , reward, done, self.info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ad0240",
   "metadata": {},
   "source": [
    "### Implementation of my DQN module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0784eacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQNetwork(nn.Module):\n",
    "    def __init__(self, lr, input_dims, fc1_dims, fc2_dims, n_actions):\n",
    "        \"\"\"Initialization of a simple neural network with 3 layers, of MLP type\n",
    "        and MSE loss\"\"\"\n",
    "        \n",
    "        super(DeepQNetwork, self).__init__()\n",
    "        self.input_dims = input_dims\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims\n",
    "        self.n_actions = n_actions\n",
    "        self.fc1 = nn.Linear(*self.input_dims, self.fc1_dims)\n",
    "        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
    "        self.fc3 = nn.Linear(self.fc2_dims, self.n_actions)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        self.loss = nn.MSELoss()\n",
    "        \n",
    "        #using the gpu\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"activation relu with the following possible actions\"\"\"\n",
    "        \n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        actions = self.fc3(x)\n",
    "\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d47fb7",
   "metadata": {},
   "source": [
    "### Implementation of my agent module\n",
    "\n",
    "Initialization of the DQN module, to learn the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4637bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, gamma, epsilon, lr, input_dims, batch_size, n_actions,\n",
    "                 max_mem_size=100, eps_end=0.05, eps_dec=5e-4):\n",
    "        \"\"\"Initialization of the hyperparameters \n",
    "            for learning and the neural network, \n",
    "            with a memory system\"\"\"\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.eps_min = eps_end\n",
    "        self.eps_dec = eps_dec\n",
    "        self.lr = lr\n",
    "        self.action_space = [i for i in range(n_actions)]\n",
    "        self.mem_size = max_mem_size\n",
    "        self.batch_size = batch_size\n",
    "        self.mem_cntr = 0\n",
    "        self.iter_cntr = 0\n",
    "        self.replace_target = 100\n",
    "\n",
    "        #initialization of the NN\n",
    "        self.Q_eval = DeepQNetwork(lr, n_actions=n_actions,\n",
    "                                   input_dims=input_dims,\n",
    "                                   fc1_dims=256, fc2_dims=256)\n",
    "        \n",
    "        self.state_memory = np.zeros((self.mem_size, *input_dims),\n",
    "                                     dtype=np.float32)\n",
    "        self.new_state_memory = np.zeros((self.mem_size, *input_dims),\n",
    "                                         dtype=np.float32)\n",
    "        self.action_memory = np.zeros(self.mem_size, dtype=np.int32)\n",
    "        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.bool)\n",
    "        \n",
    "        #For tensorboard\n",
    "        self.writer = SummaryWriter()\n",
    "        \n",
    "        self.n_iter = 0\n",
    "\n",
    "    def store_transition(self, state, action, reward, state_, terminal):\n",
    "        index = self.mem_cntr % self.mem_size\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = state_\n",
    "        self.reward_memory[index] = reward\n",
    "        self.action_memory[index] = action\n",
    "        self.terminal_memory[index] = terminal\n",
    "\n",
    "        self.mem_cntr += 1\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        #espsilon for the exploration rate\n",
    "        if np.random.random() > self.epsilon:\n",
    "            state = T.tensor([observation]).to(self.Q_eval.device)\n",
    "            actions = self.Q_eval.forward(state.float())\n",
    "            action = T.argmax(actions).item()\n",
    "        else:\n",
    "            action = np.random.choice(self.action_space)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def learn(self):\n",
    "        if self.mem_cntr < self.batch_size:\n",
    "            return\n",
    "\n",
    "        self.Q_eval.optimizer.zero_grad()\n",
    "\n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "\n",
    "        batch = np.random.choice(max_mem, self.batch_size, replace=False)\n",
    "        batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "        \n",
    "        #classic initialization for CUDA\n",
    "        state_batch = T.tensor(self.state_memory[batch]).to(self.Q_eval.device)\n",
    "        new_state_batch = T.tensor(self.new_state_memory[batch]).to(self.Q_eval.device)\n",
    "        action_batch = self.action_memory[batch]\n",
    "        reward_batch = T.tensor(self.reward_memory[batch]).to(self.Q_eval.device)\n",
    "        terminal_batch = T.tensor(self.terminal_memory[batch]).to(self.Q_eval.device)\n",
    "        \n",
    "        #the forward step\n",
    "        q_eval = self.Q_eval.forward(state_batch)[batch_index, action_batch]\n",
    "        q_next = self.Q_eval.forward(new_state_batch)\n",
    "        q_next[terminal_batch] = 0.0\n",
    "\n",
    "        q_target = reward_batch + self.gamma*T.max(q_next, dim=1)[0]\n",
    "\n",
    "        #loss calculation\n",
    "        loss = self.Q_eval.loss(q_target, q_eval).to(self.Q_eval.device)\n",
    "        \n",
    "        #the backward step and weight update\n",
    "        loss.backward()\n",
    "        self.Q_eval.optimizer.step()\n",
    "        \n",
    "        self.iter_cntr += 1\n",
    "        #manual update of the exploration rate\n",
    "        self.epsilon = self.epsilon - self.eps_dec \\\n",
    "            if self.epsilon > self.eps_min else self.eps_min\n",
    "        \n",
    "        #loss display in tensorboard\n",
    "        self.writer.add_scalar('Loss/', loss, self.n_iter)\n",
    "        self.n_iter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b69731",
   "metadata": {},
   "source": [
    "### Start the first learning, basic environment for the start\n",
    "\n",
    "Runtime can take up to 1h, if you have a GPU similar to mine, i.e. Nvidia gtx 1060 ti\n",
    "\n",
    "~~If you don't want to start the training, you have the possibility to load a model already trained further down in the notebook~~\n",
    "\n",
    "Due to the fact that the trained models take a lot of storage, I didn't put them in the git. If you want them, send me a mail and I will send them to you.\n",
    "\n",
    "[nicolaszuo.contact@gmail.com](mailto:nicolaszuo.contact@gmail.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa02971",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = CarEnv()\n",
    "agent = Agent(gamma=0.99, epsilon=1.0, batch_size=5, n_actions=3, eps_end=0.1,\n",
    "              input_dims=[480*640*3], lr=0.001, eps_dec=1e-3)\n",
    "scores, eps_history = [], []\n",
    "\n",
    "#number of iterations\n",
    "n_games = 300\n",
    "render = False\n",
    "\n",
    "start_time = time.time()\n",
    "for i in range(n_games):\n",
    "    #initialization of the episode\n",
    "    score = 0\n",
    "    done = False\n",
    "    #get the first observation\n",
    "    observation = env.reset()\n",
    "    #normalization of observations and transformation into a simple list\n",
    "    flat_observation = observation.reshape(1,-1)[0]/255.0\n",
    "    \n",
    "    while not done:\n",
    "        if render:\n",
    "            cv2.imshow(f'Agent - preview', observation)\n",
    "            cv2.waitKey(1)\n",
    "            \n",
    "        action = agent.choose_action(flat_observation.astype(float))\n",
    "        observation_, reward, done, info = env.step(action)\n",
    "        flat_observation_ = observation_.reshape(1,-1)[0]/255.0\n",
    "        score += reward\n",
    "        agent.store_transition(flat_observation.astype(float), action, reward, \n",
    "                                flat_observation_, done)\n",
    "        agent.learn()\n",
    "        flat_observation = flat_observation_\n",
    "        observation = observation_\n",
    "    scores.append(score)\n",
    "    eps_history.append(agent.epsilon)\n",
    "    \n",
    "    #average of the last 10 scores\n",
    "    avg_score = np.mean(scores[-10:])\n",
    "    if render:\n",
    "        cv2.destroyWindow(f'Agent - preview')\n",
    "    for actor in env.actor_list:\n",
    "        actor.destroy()\n",
    "    time_n = time.time() - start_time\n",
    "    print('episode ', i, 'score %.2f' % score,\n",
    "            'average score %.2f' % avg_score,\n",
    "            'epsilon %.2f' % agent.epsilon,\n",
    "             'time %.2f s' % time_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7879ea24",
   "metadata": {},
   "source": [
    "## New environment module\n",
    "\n",
    "Recalculation of the reward function, which rewards according to the distance traveled between the starting point and the current point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25de4bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOW_PREVIEW = False\n",
    "IM_WIDTH = 640\n",
    "IM_HEIGHT = 480\n",
    "MAX_EPISODE = 10\n",
    "\n",
    "\n",
    "class CarEnvDistanceReward:\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    SHOW_CAM = SHOW_PREVIEW\n",
    "    STEER_AMT = 1.0\n",
    "    im_width = IM_WIDTH\n",
    "    im_height = IM_HEIGHT\n",
    "    front_camera = None\n",
    "\n",
    "    def __init__(self, reward_function=None, verbose=0, EXPERIENCE_SECONDE = None):\n",
    "        \"\"\"Initialization of Carla client and environment components\"\"\"\n",
    "\n",
    "        self.client = carla.Client(\"localhost\", 2000)\n",
    "        self.client.set_timeout(20.0)\n",
    "        self.world = self.client.get_world()\n",
    "        self.blueprint_library = self.world.get_blueprint_library()\n",
    "        self.model_3 = self.blueprint_library.filter(\"model3\")[0]\n",
    "        self.actor_list = []\n",
    "        self.action_space = Discrete(3)\n",
    "        self.observation_space = Box(0, 255, [IM_HEIGHT,IM_WIDTH,3])\n",
    "        self.info = {'episode':0}\n",
    "        self.reward_function = reward_function\n",
    "        self.verbose = verbose\n",
    "        self.iter = 0\n",
    "        self.experience_seconde = EXPERIENCE_SECONDE\n",
    "\n",
    "    def reset(self):\n",
    "        \n",
    "        self.collision_hist = []\n",
    "        self.actor_list = []\n",
    "        self.info = {'episode':0}\n",
    "        self.iter = 0\n",
    "        \n",
    "        #reset the position of our vehicle\n",
    "        self.transform = random.choice(self.world.get_map().get_spawn_points())\n",
    "        self.vehicle = self.world.spawn_actor(self.model_3, self.transform)\n",
    "        self.actor_list.append(self.vehicle)\n",
    "\n",
    "        #reset the front rgb camera of the vehicle\n",
    "        self.rgb_cam = self.blueprint_library.find('sensor.camera.rgb')\n",
    "        self.rgb_cam.set_attribute(\"image_size_x\", f\"{self.im_width}\")\n",
    "        self.rgb_cam.set_attribute(\"image_size_y\", f\"{self.im_height}\")\n",
    "        self.rgb_cam.set_attribute(\"fov\", f\"110\")\n",
    "\n",
    "        transform = carla.Transform(carla.Location(x=2.5, z=0.7))\n",
    "        not_spawn = True\n",
    "\n",
    "        #The cases where our vehicle does not arrive to spawn, \n",
    "        #because there is already another agent on the place of the spawn\n",
    "        while not_spawn:\n",
    "            try :\n",
    "                self.sensor = self.world.spawn_actor(self.rgb_cam, transform, attach_to=self.vehicle)\n",
    "                not_spawn = False\n",
    "            except Exception:\n",
    "                not_spawn = True\n",
    "            \n",
    "        self.actor_list.append(self.sensor)\n",
    "        self.sensor.listen(lambda data: self.process_img(data))\n",
    "\n",
    "        self.vehicle.apply_control(carla.VehicleControl(throttle=0.0, brake=0.0))\n",
    "        \n",
    "        #reset of the collision sensor\n",
    "        colsensor = self.blueprint_library.find(\"sensor.other.collision\")\n",
    "        self.colsensor = self.world.spawn_actor(colsensor, transform, attach_to=self.vehicle)\n",
    "        self.actor_list.append(self.colsensor)\n",
    "        self.colsensor.listen(lambda event: self.collision_data(event))\n",
    "        \n",
    "        #as long as we have not finished converting our camera data into RGB data\n",
    "        while self.front_camera is None:\n",
    "            time.sleep(0.01)\n",
    "\n",
    "        self.episode_start = time.time()\n",
    "        self.vehicle.apply_control(carla.VehicleControl(throttle=0.0, brake=0.0))\n",
    "        \n",
    "        #initial position of the vehicle\n",
    "        self.initial_Location = self.vehicle.get_location()\n",
    "        \n",
    "        return self.front_camera\n",
    "\n",
    "    def collision_data(self, event):\n",
    "        self.collision_hist.append(event)\n",
    "\n",
    "    def process_img(self, image):\n",
    "        \"\"\"conversion of the raw image that Carla sends us from the RGB sensor, \n",
    "        into data of type (HEIGHT, WIDTH, 3)\"\"\"\n",
    "        \n",
    "        i = np.array(image.raw_data)\n",
    "        #print(i.shape)\n",
    "        i2 = i.reshape((self.im_height, self.im_width, 4))\n",
    "        i3 = i2[:, :, :3]\n",
    "        if self.SHOW_CAM:\n",
    "            cv2.imshow(\"\", i3)\n",
    "            cv2.waitKey(1)\n",
    "        self.front_camera = i3\n",
    "\n",
    "    def step(self, action):\n",
    "        if action == 0:\n",
    "            #turn left\n",
    "            self.vehicle.apply_control(carla.VehicleControl(throttle=1.0, steer=-1*self.STEER_AMT))\n",
    "        elif action == 1:\n",
    "            #continue straight on\n",
    "            self.vehicle.apply_control(carla.VehicleControl(throttle=1.0, steer= 0))\n",
    "        elif action == 2:\n",
    "             #turn left\n",
    "            self.vehicle.apply_control(carla.VehicleControl(throttle=1.0, steer=1*self.STEER_AMT))\n",
    "\n",
    "        #v = self.vehicle.get_velocity()\n",
    "        #kmh = int(3.6 * math.sqrt(v.x**2 + v.y**2 + v.z**2))\n",
    "\n",
    "        if len(self.collision_hist) != 0:\n",
    "            #if collision\n",
    "            done = True\n",
    "            reward = -50\n",
    "            if self.verbose == 1:\n",
    "                print(\"Collision detected !\")\n",
    "        else :\n",
    "            done = False\n",
    "            #calculation of the distance from the starting point\n",
    "            distance = self.vehicle.get_location().distance(self.initial_Location)\n",
    "            if self.reward_function == None : \n",
    "                reward = distance\n",
    "            else:\n",
    "                reward = self.reward_function(distance)\n",
    "            if self.verbose == 1:\n",
    "                print(\"Obtain \"+str(reward)+\" reward, distance = \"+str(distance))\n",
    "        \n",
    "        if self.experience_seconde != None :\n",
    "            #iteration in second grade\n",
    "            if time.time()-self.episode_start > self.experience_seconde:\n",
    "                done = True\n",
    "        else :\n",
    "            #iteration by number of steps\n",
    "            if self.iter > MAX_EPISODE-1 :\n",
    "                done = True\n",
    "            self.iter += 1\n",
    "        self.info['episode'] += 1\n",
    "        \n",
    "        return self.front_camera , reward, done, self.info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d293f4",
   "metadata": {},
   "source": [
    "##### Test with a reward function that rewards more the fact of covering a greater distance, function of exponential type, for positive x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f8eda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_function(x):\n",
    "    return (x**3) / 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f68e1b",
   "metadata": {},
   "source": [
    "### Start learning with the new environment\n",
    "\n",
    "Runtime can take up to 1h, if you have a GPU similar to mine, i.e. Nvidia gtx 1060 ti\n",
    "\n",
    "~~If you don't want to start the training, you have the possibility to load a model already trained further down in the notebook~~\n",
    "\n",
    "Due to the fact that the trained models take a lot of storage, I didn't put them in the git. If you want them, send me a mail and I will send them to you.\n",
    "\n",
    "[nicolaszuo.contact@gmail.com](mailto:nicolaszuo.contact@gmail.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be0e63e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = CarEnvDistanceReward()\n",
    "#env = CarEnvDistanceReward(reward_function=reward_function)\n",
    "agent = Agent(gamma=0.99, epsilon=1.0, batch_size=5, n_actions=3, eps_end=0.1,\n",
    "              input_dims=[480*640*3], lr=0.001, eps_dec=1e-3)\n",
    "scores, eps_history = [], []\n",
    "\n",
    "#number of iterations\n",
    "n_games = 300\n",
    "render = False\n",
    "\n",
    "start_time = time.time()\n",
    "for i in range(n_games):\n",
    "    #initialization of the episode\n",
    "    score = 0\n",
    "    done = False\n",
    "    #get the first observation\n",
    "    observation = env.reset()\n",
    "    #normalization of observations and transformation into a simple list\n",
    "    flat_observation = observation.reshape(1,-1)[0]/255.0\n",
    "    while not done:\n",
    "        action = agent.choose_action(flat_observation.astype(float))\n",
    "        if render:\n",
    "            cv2.imshow(f'Agent - preview', observation)\n",
    "            cv2.waitKey(1)\n",
    "        \n",
    "        observation_, reward, done, info = env.step(action)\n",
    "        flat_observation_ = observation_.reshape(1,-1)[0]/255.0\n",
    "        score += reward\n",
    "        agent.store_transition(flat_observation.astype(float), action, reward, \n",
    "                                flat_observation_, done)\n",
    "        agent.learn()\n",
    "        flat_observation = flat_observation_\n",
    "        observation = observation_\n",
    "    scores.append(score)\n",
    "    eps_history.append(agent.epsilon)\n",
    "\n",
    "    avg_score = np.mean(scores[-10:])\n",
    "    if render:\n",
    "        cv2.destroyWindow(f'Agent - preview')\n",
    "    for actor in env.actor_list:\n",
    "        actor.destroy()\n",
    "    time_n = time.time() - start_time\n",
    "    print('episode ', i, 'score %.2f' % score,\n",
    "            'average score %.2f' % avg_score,\n",
    "            'epsilon %.2f' % agent.epsilon,\n",
    "             'time %.2f s' % time_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50835e3",
   "metadata": {},
   "source": [
    "### Display of model data during training\n",
    "\n",
    "(note: You can also see the evolution of the loss, during the training with TensorBoard)\n",
    "\n",
    "with the command : **tensorboard --logdir runs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f9ec35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "start_time = time.time()\n",
    "x = [i+1 for i in range(n_games)]\n",
    "filename = 'data/DQN-EnvDistanceReward-'+str(n_games)+'-'+str(start_time)+'.jpg'\n",
    "\n",
    "def plotLearning(x, scores, epsilons, filename, lines=None):\n",
    "    fig=plt.figure()\n",
    "    ax=fig.add_subplot(111, label=\"1\")\n",
    "    ax2=fig.add_subplot(111, label=\"2\", frame_on=False)\n",
    "\n",
    "    ax.plot(x, epsilons, color=\"C0\")\n",
    "    ax.set_xlabel(\"Game\", color=\"C0\")\n",
    "    ax.set_ylabel(\"Epsilon\", color=\"C0\")\n",
    "    ax.tick_params(axis='x', colors=\"C0\")\n",
    "    ax.tick_params(axis='y', colors=\"C0\")\n",
    "\n",
    "    N = len(scores)\n",
    "    running_avg = np.empty(N)\n",
    "    for t in range(N):\n",
    "        running_avg[t] = np.mean(scores[max(0, t-20):(t+1)])\n",
    "\n",
    "    ax2.plot(x, running_avg, color=\"C1\")\n",
    "\n",
    "    ax2.axes.get_xaxis().set_visible(False)\n",
    "    ax2.yaxis.tick_right()\n",
    "    ax2.set_ylabel('Score', color=\"C1\")\n",
    "    ax2.yaxis.set_label_position('right')\n",
    "    ax2.tick_params(axis='y', colors=\"C1\")\n",
    "\n",
    "    if lines is not None:\n",
    "        for line in lines:\n",
    "            plt.axvline(x=line)\n",
    "\n",
    "    plt.savefig(filename)\n",
    "    \n",
    "plotLearning(x, scores, eps_history, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7ed47c",
   "metadata": {},
   "source": [
    "### Saving the model\n",
    "\n",
    "If you have trained your model before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f336d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "savepath = Path('model/model_DQN_EnvDistanceReward_WithRewardFunction_{}.pch'.format(str(n_games)+'-'+str(start_time)))\n",
    "\n",
    "with savepath.open('wb') as file:\n",
    "    torch.save(agent.Q_eval, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9f335e",
   "metadata": {},
   "source": [
    "### Potential model loading\n",
    "\n",
    "Due to the fact that the trained models take a lot of storage, I didn't put them in the git. If you want them, send me a mail and I will send them to you.\n",
    "\n",
    "[nicolaszuo.contact@gmail.com](mailto:nicolaszuo.contact@gmail.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1f14d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\"\"\"\n",
    "model learned about the environment with the rewards depending on the distance,\n",
    "with 600 iterations, the agent has learned to turn in circles\n",
    "\"\"\"\n",
    "#savepath = Path('model/model_DQN_EnvDistanceReward_600-1635352803.4654603.pch')\n",
    "\n",
    "\"\"\"\n",
    "#model learned with the same environment but with the cube function,\n",
    "#where the distance is very important,\n",
    "#the vehicle learned to go straight\n",
    "\"\"\"\n",
    "#savepath = Path('model/model_DQN_EnvDistanceReward_WithRewardFunction_300-1635362488.656119.pch')\n",
    "\n",
    "\"\"\"\n",
    "#model learned about the environment with the rewards depending on the distance,\n",
    "#with 300 iterations, the agent has learned to turn in circles \n",
    "#and the beginning of a collision avoidance behaviour.\n",
    "#but with the delay due to the latency during learning, the rendering is not perfect at all\n",
    "\"\"\"\n",
    "#savepath = Path('model/model_DQN_EnvDistanceReward_300-1635331329.0202696.pch')\n",
    "\n",
    "with savepath.open('rb') as file:\n",
    "    model = T.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320c9518",
   "metadata": {},
   "source": [
    "### If the model is already present\n",
    "\n",
    "If you have launched the learning cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52f91e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = agent.Q_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5350b3e5",
   "metadata": {},
   "source": [
    "## Application of the model learned on the simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca07b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode = 5\n",
    "render = True\n",
    "#env = CarEnv()\n",
    "env = CarEnvDistanceReward(EXPERIENCE_SECONDE= 10)\n",
    "start_time = time.time()\n",
    "for i in range(episode):\n",
    "    score = 0\n",
    "    done = False\n",
    "    observation = env.reset()\n",
    "    flat_observation = observation.reshape(1,-1)[0]/255.0\n",
    "    try : \n",
    "        while not done:\n",
    "\n",
    "            if render:\n",
    "                cv2.imshow(f'Agent - preview', observation)\n",
    "                cv2.waitKey(1)\n",
    "\n",
    "            data = T.tensor(flat_observation).float()\n",
    "\n",
    "            action = model.forward(data)\n",
    "            action = action.detach().numpy().argmax()\n",
    "\n",
    "            observation_, reward, done, info = env.step(action)\n",
    "            flat_observation_ = observation_.reshape(1,-1)[0]/255.0\n",
    "            score += reward\n",
    "            flat_observation = flat_observation_\n",
    "            observation = observation_\n",
    "\n",
    "    finally : \n",
    "        if render:\n",
    "            cv2.destroyWindow(f'Agent - preview')\n",
    "        for actor in env.actor_list:\n",
    "            actor.destroy()\n",
    "        time_n = time.time() - start_time\n",
    "        print('episode ', i, 'score %.2f' % score,'time %.2f s' % time_n)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
